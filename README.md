# ğŸ CricPulse â€” Real-Time Cricket Data Pipeline with Airflow, Kafka & PySpark

CricPulse is an end-to-end **real-time cricket analytics pipeline** that simulates live match streams, processes ball-by-ball data using **Kafka + PySpark**, and orchestrates workflows using **Apache Airflow**.  
The system computes key cricket metrics like **strike rate, economy, team run rate**, and more â€” enabling contextual anomaly detection and insights.

---

## ğŸš€ Tech Stack
- **Apache Kafka** â†’ Real-time message streaming
- **Apache PySpark** â†’ Stream processing & feature computation
- **Apache Airflow** â†’ Workflow orchestration & scheduling
- **PostgreSQL** â†’ Airflow metadata database
- **Docker Compose** â†’ Containerized environment
- **Python 3.11+**

---

## ğŸ§© Project Architecture
```
CricPulse/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                â†’ Airflow DAGs (workflow definitions)
â”‚   â”œâ”€â”€ logs/                â†’ Airflow runtime logs
â”‚   â”œâ”€â”€ plugins/             â†’ Custom Airflow plugins
â”‚   â”œâ”€â”€ docker-compose.yml   â†’ Airflow + PostgreSQL + Scheduler setup
â”‚
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ docker-compose-kafka.yml â†’ Kafka + Zookeeper setup
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ kafka_producer.py    â†’ Sends cricket event JSONs to Kafka topic
â”‚   â”œâ”€â”€ pyspark_consumer.py  â†’ Consumes Kafka stream & processes with PySpark
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/              â†’ Raw streamed data written by Spark
â”‚   â”œâ”€â”€ ipl25/               â†’ Sample IPL JSON files (input for producer)
â”‚
â”œâ”€â”€ checkpoints/             â†’ Spark streaming checkpoints
â”‚
â”œâ”€â”€ requirements.txt         â†’ Python dependencies
â”œâ”€â”€ .gitignore               â†’ Ignored files & folders
â””â”€â”€ README.md                â†’ Project documentation
```

---

## ğŸ§  Core Features
âœ… Real-time data ingestion from simulated cricket JSON feeds  
âœ… Kafka-based streaming between producer and PySpark consumer  
âœ… Airflow DAGs to orchestrate ingestion, processing, and anomaly detection  
âœ… Spark computation of:
- Batsman Strike Rate  
- Bowler Economy  
- Team Run Rate 
âœ… PostgreSQL backend for Airflow (via Docker Compose)

---

## ğŸ§ª Setup Guide â€” How to Run CricPulse

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/<your-username>/CricPulse.git
cd CricPulse
```

### 2ï¸âƒ£ Start Airflow (Webserver + Scheduler + PostgreSQL)
```bash
cd airflow
docker compose up -d
```
Visit **http://localhost:8080**  
Default login (after creating a user):
```
Username: admin
Password: admin
```

### 3ï¸âƒ£ Start Kafka + Zookeeper
```bash
cd ../kafka
docker compose -f docker-compose-kafka.yml up -d
```

### 4ï¸âƒ£ Produce Match Data
```bash
cd ../scripts
python kafka_producer.py
```

### 5ï¸âƒ£ Consume Stream with PySpark
```bash
python pyspark_consumer.py
```

Youâ€™ll see computed metrics written to `/data/bronze/raw_events/` and streaming checkpoints saved under `/checkpoints/`.

---

## ğŸŒŸ Why This Project Stands Out
- **End-to-End Streaming System**: Integrates ingestion â†’ processing â†’ orchestration seamlessly.  
- **Portfolio-Ready**: Demonstrates strong Big Data, Cloud, and Realtime Engineering skills.  
- **Modular Design**: Each component (Airflow, Kafka, PySpark) is isolated for clarity.  

---

## âš™ï¸ Future Enhancements
- Add **Delta Lake** for bronze â†’ silver â†’ gold transformation  
- Deploy **Airflow on EC2 / GCP Composer**  
- Integrate **ML anomaly detection** using PySpark MLlib  
- Build a **Streamlit dashboard** for live match insights  

---
